Stop words: The words that are used frequently but doesnot contribute much in the meaning of the sentence. They are senteiment neutral. We drop them.
Structured: CSV and PRK
Unstructred: text, Binary data or no delimers or no indication of rows like social media post and email put in file.
Data set is from : SMSSSpamCollection.tsv Its the collection of the text data.

Why Are Regular Expressions Useful?
• Identifying whitespace between words/tokens
• Identifying/creating delimiters or end-of-line escape characters
• Removing punctuation or numbers from your text
• Cleaning HTML tags from text
• Identifying some textual patterns you're interested in


For tokenization we use two methods:
1. Findall(): Findall the words while ignoring the spaces and special characters.
2. Split(): Split the words without actually knowing the words.

\W or \w is related to words.
\S or \s is related to spaces.

1st:Raw text: Model can´t distingused it 
2nd:Tokenize text: (Tell the model what to look at)
3rd: Clean data: Remove stop words/puncuation and stemming etc.
4th: Vectorize: Converting text to numeric represntation of that text.
5th: Machine learning algorithm: Fit/train model.
6th: Apply on the situation (ex: Spam to filter email.) 

Stemming: The process of reducing inflected (or derived) words to their word stem or root.
cruudely chopping off the end of the word to only the base.
It reduces the corpus of word the model is exposed to.
explicitely correlates words with the similar meaning.
Example:
stemming/stemmed -> stem
berries/berry -> berri
use crude approach and thus not always correct.
Types: Porter based stemmer, snowball stemmer, lanchaster stemmer, Regex-based stemmer.

lamitizing: Process of groring togather the infected forms of the words so they can be analyzed as a single term identified by the word lemma.
tYPE, Typing are the same lemma.
Using vocabulary analysis of the word aiming to remove infectional endings to return the dictionary form of a word.

Differnec between lemmatizing and stemming
stemming: Faster as it simply chops off the end of the word using heuristics, withour any understadng of the context in which the word is used.
lemmatizing: More accurate as it used more informed analysis to create group of words with similar meaning based on the context around the word. More computational expensive as it also take in account the context.