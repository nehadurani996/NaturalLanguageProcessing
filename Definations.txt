Stop words: The words that are used frequently but doesnot contribute much in the meaning of the sentence. They are senteiment neutral. We drop them.
Structured: CSV and PRK
Unstructred: text, Binary data or no delimers or no indication of rows like social media post and email put in file.
Data set is from : SMSSSpamCollection.tsv Its the collection of the text data.

Why Are Regular Expressions Useful?
• Identifying whitespace between words/tokens
• Identifying/creating delimiters or end-of-line escape characters
• Removing punctuation or numbers from your text
• Cleaning HTML tags from text
• Identifying some textual patterns you're interested in


For tokenization we use two methods:
1. Findall(): Findall the words while ignoring the spaces and special characters.
2. Split(): Split the words without actually knowing the words.

\W or \w is related to words.
\S or \s is related to spaces.

1st:Raw text: Model can´t distingused it 
2nd:Tokenize text: (Tell the model what to look at)
3rd: Clean data: Remove stop words/puncuation and stemming etc.
4th: Vectorize: Converting text to numeric represntation of that text.
5th: Machine learning algorithm: Fit/train model.
6th: Apply on the situation (ex: Spam to filter email.) 

Stemming: The process of reducing inflected (or derived) words to their word stem or root.
cruudely chopping off the end of the word to only the base.
It reduces the corpus of word the model is exposed to.
explicitely correlates words with the similar meaning.
Example:
stemming/stemmed -> stem
berries/berry -> berri
use crude approach and thus not always correct.
Types: Porter based stemmer, snowball stemmer, lanchaster stemmer, Regex-based stemmer.

lamitizing: Process of groring togather the infected forms of the words so they can be analyzed as a single term identified by the word lemma.
tYPE, Typing are the same lemma.
Using vocabulary analysis of the word aiming to remove infectional endings to return the dictionary form of a word.

Differnec between lemmatizing and stemming
stemming: Faster as it simply chops off the end of the word using heuristics, withour any understadng of the context in which the word is used.
lemmatizing: More accurate as it used more informed analysis to create group of words with similar meaning based on the context around the word. More computational expensive as it also take in account the context.

Vectorize:
encoding text as integer to create a feature vector.
Feature vector: 
A n-dimensional vector of the numeric features that represent some objects.
Raw data output of the countvectorizer is called sparse matrix which is a matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements.
N-grams:
Create a document-term matrix where counts still occpy the cell but instead of the columns representing single term,
they rrepresent all combination of adjacent words of length n in your text.

TF-ITF equation:
create doc term message.
wi,j number of occurance of word
N= total number of terms.

Transformation Process:
1. Determine what range of exponents to test
2. Apply each transformation to each value of your chosen feature
3. Use some criteria to determine which of the transformations yield the best distribution.

K-fold cross validation:
The full data set id divided into k-subsets and holdout method is repeated k times. Each time, one of the k-subsets is used as the test set and the other k-1 subeste are put tofather to be used to train the model.