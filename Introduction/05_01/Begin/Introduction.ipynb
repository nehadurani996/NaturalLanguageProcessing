{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction or week 1 work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After opening the file the 1st thing i noticed it that we need to manage the upper portion of the bok having title and oher things etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274951"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"PeterPan.txt\", \"r\", encoding=\"utf-8-sig\")\n",
    "t = f.read()\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The charater length of the text in the book is 274951 which means there are total 274951 characters in the whole book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK TOKANIZERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the book to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2563"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.tokenize as tok\n",
    "sents = tok.sent_tokenize(t, language= \"english\")\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes the number corrosponds to the number of my expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Peter Pan, by James M. Barrie\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.\n",
      "------------------\n",
      "You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org.\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(sents[0])\n",
    "print (\"------------------\")\n",
    "print(sents[1])\n",
    "print (\"------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Project Gutenberg eBook of Peter Pan, by James M. Barrie This eBook is for the use of anyone any\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "t = re.sub(r\"\\n+\", \" \", t) #re.sub() is subsitude method in regular expression.\n",
    "print(t[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61946"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tok.word_tokenize(t, language=\"english\")\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274951\n",
      "['The', 'Project', 'Gutenberg', 'eBook', 'of', 'Peter', 'Pan', ',', 'by', 'James', 'M.', 'Barrie', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"peterpan.txt\", \"r\", encoding=\"utf-8-sig\")\n",
    "t = f.read()\n",
    "tokens = tok.word_tokenize(t, language=\"english\")\n",
    "print(len(t))\n",
    "print(tokens[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency distribution and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2329), ('“', 1464), ('”', 1464), ('to', 1222), ('a', 959), ('of', 923), ('was', 923), ('he', 878), ('in', 698), ('it', 661)]\n",
      "361\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "c = collections.Counter(tokens) # The Counter class is used to create a dictionary-like object \n",
    "del c[\"and\"]\n",
    "print(c.most_common(10))\n",
    "print(c[\"with\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "clean_text= remove_punctuation(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2329), ('“', 1464), ('”', 1464), ('and', 1420), ('to', 1222), ('a', 959), ('of', 923), ('was', 923), ('he', 878), ('in', 698)]\n",
      "361\n"
     ]
    }
   ],
   "source": [
    "len(clean_text)\n",
    "tokens = tok.word_tokenize(clean_text, language=\"english\")\n",
    "c = collections.Counter(tokens) # The Counter class is used to create a dictionary-like object \n",
    "print(c.most_common(10))\n",
    "print(c[\"with\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2329), ('“', 1464), ('”', 1464), ('and', 1420), ('to', 1222), ('a', 959), ('of', 923), ('was', 923), ('he', 878), ('in', 698)]\n",
      "  Word  Frequency\n",
      "0  the       2329\n",
      "1    “       1464\n",
      "2    ”       1464\n",
      "3  and       1420\n",
      "4   to       1222\n",
      "5    a        959\n",
      "6   of        923\n",
      "7  was        923\n",
      "8   he        878\n",
      "9   in        698\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = c.most_common(10)\n",
    "print (data)\n",
    "df= pd.DataFrame(data, columns= [\"Word\",\"Frequency\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens=  54354\n",
      "Type of tokens=  5836\n",
      "Ratio=  9.313570938999314\n"
     ]
    }
   ],
   "source": [
    "tokens = tok.word_tokenize(clean_text, language=\"english\")\n",
    "print(\"Tokens= \", len(tokens))\n",
    "print(\"Type of tokens= \", len(set(tokens)))\n",
    "print(\"Ratio= \",len(tokens)/len(set(tokens)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
